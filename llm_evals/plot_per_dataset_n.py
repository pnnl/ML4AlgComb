"""
Creates one plot per (dataset, n) pair from Inspect evaluation logs produced by
the tasks in llm_evaluation.py (algcomb_classification, algcomb_program_synthesis).
For each dataset-n pair, the plot shows accuracy vs. few-shot count, colored by model.

Example Usage:
    python plot_per_dataset_n.py --logdir logs_dir

Where logs_dir is a directory containing one or more Inspect log files (*.json)
generated by, e.g.:
    inspect eval experiment_runner.py --json > logs_part1.json
    inspect eval llm_evaluation.py@algcomb_classification --json > logs_part2.json
(and so on). Just place them all in logs_dir, and they will be combined.

This script uses `read_eval_log` from `inspect_ai.log` instead of raw JSON.
"""

import os
import argparse
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from inspect_ai.log import read_eval_log

sns.set_style("whitegrid")

def parse_inspect_log_file(log_path):
    """
    Reads a single Inspect log file via read_eval_log and extracts records for each
    task (or single task). Returns a list of dictionaries with:
      {
          "dataset": dataset_name,
          "n": n_value,
          "few_shot": few_shot_count,
          "model": model,
          "accuracy": final_score
      }

    We attempt to locate 'accuracy' or 'score' or 'mean' in the log's metrics 
    to use as the final score.
    """
    records = []
    log_file = read_eval_log(log_path)
    if not log_file or not log_file.results:
        return records

    # Identify the top-level model name, if any, from the log
    # (Because a multi-task run might have used the same model across tasks)
    top_level_model = getattr(log_file.eval, "model", "unknown_model")

    # If it's a multi-task log, we might have multiple tasks in log_file.results.task_results
    # If not multi-task, we might just have log_file.results.metrics, etc.
    task_results = getattr(log_file.results, "task_results", None)

    def extract_final_score(metrics_dict):
        """Helper to find a final score-like metric (accuracy, score, or mean)."""
        for possible_key in ["accuracy", "score", "mean"]:
            if possible_key in metrics_dict:
                return metrics_dict[possible_key].value
        # If none of these are found, return None
        return None

    if task_results:
        # Multi-task scenario
        for tresult in task_results:
            params = getattr(tresult, "task_args", {})
            dataset = params.get("dataset", "unknown_dataset")
            n_value = params.get("n", -1)
            few_shot = params.get("few_shot_count", 0)
            # Fall back to top_level_model if not present in tresult
            model = getattr(log_file.eval, "model", "unknown_model")

            # Attempt to get final score from task-level metrics
            final_score = extract_final_score(tresult.metrics)
            # NEW: Capture whether CoT was used
            cot = params.get("use_chain_of_thought", False)

            records.append({
                "dataset": dataset,
                "n": n_value,
                "few_shot": few_shot,
                "model": model,
                "accuracy": final_score,
                "use_chain_of_thought": cot
            })
    else:
        # Single-task scenario
        params = getattr(log_file.eval, "task_args", {})
        dataset = params.get("dataset", "unknown_dataset")
        n_value = params.get("n", -1)
        few_shot = params.get("few_shot_count", 0)
        model = top_level_model

        # Attempt to get final score from top-level metrics
        final_score = extract_final_score(log_file.results.metrics)
        # NEW: Capture whether CoT was used
        cot = params.get("use_chain_of_thought", False)

        records.append({
            "dataset": dataset,
            "n": n_value,
            "few_shot": few_shot,
            "model": model,
            "accuracy": final_score,
            "use_chain_of_thought": cot
        })

    return records


def parse_inspect_logs_from_dir(log_dir):
    """
    Reads all .json files in log_dir, parses them using parse_inspect_log_file,
    and returns a combined DataFrame with columns: dataset, n, few_shot, model, accuracy.
    """
    all_records = []
    for file_name in os.listdir(log_dir):
        if file_name.endswith(".json") or file_name.endswith(".eval"):
            file_path = os.path.join(log_dir, file_name)
            new_records = parse_inspect_log_file(file_path)
            all_records.extend(new_records)

    df = pd.DataFrame(all_records)
    return df


def main():
    """
    Reads Inspect log files in a directory, combines them into a DataFrame, then generates
    a per-(dataset, n) line plot of accuracy vs. few-shot count, colored by model.
    """
    parser = argparse.ArgumentParser(description="Plots Inspect logs (accuracy vs. few_shot) using read_eval_log.")
    parser.add_argument("--logdir", type=str, default="logs_dir", help="Directory containing the JSON logs from Inspect.")
    parser.add_argument("--task_name", type=str, default="n-shot classification", help="Name of the task to plot.")
    args = parser.parse_args()

    # Parse all logs in the directory
    df = parse_inspect_logs_from_dir(args.logdir)

    # If no logs found, exit gracefully
    if df.empty:
        print(f"No .json logs found in '{args.logdir}' or no metrics data extracted.")
        return

    # Sort by few_shot for plotting convenience
    df = df.sort_values("few_shot")

    # For each dataset, and each value of n, create and save a plot
    for dataset in df["dataset"].unique():
        ds_subset = df[df["dataset"] == dataset]
        for n_val in ds_subset["n"].unique():
            subset = ds_subset[ds_subset["n"] == n_val]

            if subset.empty:
                continue

            # Group by few_shot and model, and get the max accuracy for each group
            subset = subset.groupby(['few_shot', 'model', 'use_chain_of_thought'])['accuracy'].max().reset_index()

            # Create the plot
            plt.figure(figsize=(6, 4))
            sns.lineplot(
                data=subset,
                x="few_shot",
                y="accuracy",
                hue="model",
                style="use_chain_of_thought",
                markers=True,
                dashes=False,
                errorbar=None
            )
            plt.title(f"Dataset = {dataset}, n = {n_val}, task name = {args.task_name}")
            plt.xlabel("Few-Shot Examples")
            plt.ylabel("Accuracy")
            plt.ylim(0, 1)  # classification accuracies or normalized scores typically in [0, 1]
            plt.legend(loc="best")
            plt.tight_layout()

            # Save as PNG
            out_filename = f"{dataset}_n={n_val}_task_name={args.task_name}.png"
            plt.savefig(out_filename)
            plt.close()
            print(f"Saved plot: {out_filename}")

    print("Done! Plots created for each (dataset, n) pair.")


if __name__ == "__main__":
    main() 